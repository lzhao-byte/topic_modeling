{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('gatech': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6c1155f863d4337240da029764ab92710ddb55e52734e0e03582fbcfef0cf508"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "import sqlite3 as sql\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links = pd.read_csv('../papers_info/jits/jits_paper_links.csv', encoding = 'latin1')\n",
    "papers = []\n",
    "totalkws, totalkwids = [], []\n",
    "abstracts = {}\n",
    "authors = []\n",
    "reftitles, refjournals, refids = [], [], []\n",
    "\n",
    "for row in links.iterrows():\n",
    "    paper = row[1]\n",
    "    paper_id, doi, pdf_url, title = paper.Urls.split(\"/\")[-1], \"/\".join(paper.Urls.split(\"/\")[-2:]), paper.Urls, paper.Titles\n",
    "    issue_no, year = paper.BookId, paper.BookId.split('-')[0]\n",
    "    \n",
    "    # get page\n",
    "    page = requests.get(pdf_url, headers = headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')  \n",
    "    \n",
    "    if \"Editorial\" == soup.find(\"h3\").string.strip() or \"Editorials\" == soup.find(\"h3\").string.strip():\n",
    "        continue  \n",
    "\n",
    "    # page information\n",
    "    pps = soup.find('span', {'class': 'contentItemPageRange'}).string\n",
    "    try:\n",
    "        startpp, endpp = pps.strip().split('-')[0].split()[-1], pps.strip().split('-')[1]\n",
    "    except:\n",
    "        startpp, endpp = 0, 0\n",
    "\n",
    "    # citedby\n",
    "    try:\n",
    "        cited = int(soup.find('a', {'class': 'crossRef'}).find('span', {'class': 'value'}).string)\n",
    "    except:\n",
    "        cited = 0\n",
    "\n",
    "    count = 0\n",
    "    #author information\n",
    "    for au in soup.find('div', {'class': 'hlFld-ContribAuthor'}).find_all('a', {'class': \"entryAuthor\"}):\n",
    "        name = au.contents[0]\n",
    "        try:\n",
    "            affiliation = au.find('span').string\n",
    "            author_info = (paper_id, count, name, 0, affiliation)\n",
    "        except AttributeError:\n",
    "            try: \n",
    "                affiliation = au.find_all('span')\n",
    "                author_info = (paper_id, count, name, 0, affiliation[0].contents[0])\n",
    "            except TypeError:\n",
    "                author_info = (paper_id, count, name, 0, 'NA')\n",
    "        count += 1\n",
    "        authors.append(author_info)\n",
    "\n",
    "    # abstract\n",
    "    try:\n",
    "        abstract = soup.find('div', {'class':  'hlFld-Abstract'}).find('p')\n",
    "        if abstract.string.strip() == 'Abstract':\n",
    "            abstract = abstract.find_next('p')\n",
    "        abstracts.update({paper_id: abstract.string.strip()})\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "    papers.append((paper_id, doi, title, year, startpp, endpp, cited, issue_no, pdf_url))\n",
    "\n",
    "    # keywords\n",
    "    kws = []\n",
    "    try:\n",
    "        for kw in soup.find('div', {'class':  'hlFld-KeywordText'}).find_all('a'):\n",
    "            kws += kw.contents\n",
    "        if len(kws) > 0:\n",
    "            kw_ids = [paper_id] * len(kws)\n",
    "            totalkws.append(kws)\n",
    "            totalkwids.append(kw_ids)\n",
    "    except:\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # references\n",
    "    ref_url = pdf_url.replace('full', 'ref')\n",
    "    refpage = requests.get(ref_url, headers = headers)\n",
    "    refsoup = BeautifulSoup(refpage.content, 'html.parser')   \n",
    "    ref_journals, ref_titles = [], []\n",
    "    for li in refsoup.find('ul', {'class': 'references'}).find_all('li'):\n",
    "        try:\n",
    "            ref_title = li.find('span', {'class': 'NLM_article-title'}).string\n",
    "            ref_titles.append(ref_title)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        try:\n",
    "            ref_journal = li.find('i').string\n",
    "            ref_journals.append(ref_journal)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                ref_journal = li.find('span', {'class': 'NLM_conf-name'}).string\n",
    "                ref_journals.append(ref_journal)\n",
    "            except AttributeError:\n",
    "                ref_journals.append(\"NA\")\n",
    "    \n",
    "    ref_ids = [paper_id] * len(ref_titles)\n",
    "    refids.append(ref_ids)\n",
    "    reftitles.append(ref_titles)\n",
    "    refjournals.append(ref_journals)\n",
    "\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.DataFrame.from_dict(abstracts, orient = 'index').reset_index()\n",
    "abstracts.columns = ['paper_id', 'abstract']\n",
    "abstracts.to_csv('../papers_info/jits/jits_paper_abstracts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.DataFrame(authors, columns = ['paper_id', 'author_order', 'author_name', 'author_id', 'affiliation'])\n",
    "authors.to_csv('../papers_info/jits/jits_paper_authors.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.DataFrame(papers, columns = ['paper_id', 'doi', 'title', 'year', 'startpp', 'endpp', 'cited', 'issue_no', 'pdf_url'])\n",
    "papers.to_csv('../papers_info/jits/jits_paper_info.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_kws = pd.DataFrame()\n",
    "\n",
    "for i in range(len(totalkws)):\n",
    "    temp = pd.DataFrame(zip(totalkws[i], totalkwids[i]))\n",
    "    total_kws = pd.concat([total_kws, temp])\n",
    "\n",
    "total_kws.columns = ['keyword_terms', 'paper_id']\n",
    "total_kws.to_csv('../papers_info/jits/jits_paper_keywords.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = pd.DataFrame()\n",
    "\n",
    "for i in range(len(refids)):\n",
    "    temp = pd.DataFrame(zip(refids[i], reftitles[i], refjournals[i]))\n",
    "    refs = pd.concat([refs, temp])\n",
    "refs.columns = ['paper_id', 'ref_title', 'ref_journal']\n",
    "refs.to_csv('../papers_info/jits/jits_paper_references.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 0, 'Cindy Cappelle', 0, 'Laboratoire Systèmes et Transports , Université de Technologie de Belfort-Montbéliard ,')\n(1, 1, 'Maan El Badaoui El Najjar', 0, 'Laboratoire Lorrain de Recherche en Informatique et ses Applications , Institut National de Recherche en Informatique et Automatique ,')\n(1, 2, 'Denis Pomorski', 0, \"Laboratoire d'Automatique, Génie Informatique et Signal , Université de Lille Nord de France ,\")\n(1, 3, 'François Charpillet', 0, 'Laboratoire Lorrain de Recherche en Informatique et ses Applications , Institut National de Recherche en Informatique et Automatique ,')\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://www.tandfonline.com/doi/full/10.1080/15472450903385999\"\n",
    " # get page\n",
    "paper_id=1\n",
    "page = requests.get(test_url, headers = headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')    \n",
    "count = 0\n",
    "#author information\n",
    "\n",
    "for author in soup.find('div', {'class': 'hlFld-ContribAuthor'}).find_all('a'):\n",
    "    name = author.contents[0]\n",
    "    try:\n",
    "        affiliation = author.find('span').string\n",
    "        author_info = (paper_id, count, name.strip(), 0, affiliation.strip())\n",
    "    except AttributeError:\n",
    "        try: \n",
    "            affiliation = author.find_all('span')\n",
    "            author_info = (paper_id, count, name.strip(), 0, affiliation[0].contents[0].strip())\n",
    "        except TypeError:\n",
    "            author_info = (paper_id, count, name.strip(), 0, \"NA\")\n",
    "    count += 1\n",
    "    print(author_info)"
   ]
  }
 ]
}